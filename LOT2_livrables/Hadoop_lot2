# MapReduce
hdfs dfs -rm -r -f /user/root/output/lot2
hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \
  -D mapreduce.job.name="LOT2 sum&avg sans timbrecli" \
  -D mapreduce.job.reduces=1 \
  -input /user/root/dw_data/DW.csv \
  -output /user/root/output/lot2 \
  -mapper "python3 mapper_lot2.py" \
  -reducer "./reduce_lot2.sh" \
  -file /root/mapper_lot2.py \
  -file /root/reducer_lot2.py \
  -file /root/reduce_lot2.sh

# Top 100 (sum desc puis avg desc)
hdfs dfs -cat /user/root/output/lot2/part-* | sort -t$'\t' -k3,3nr -k4,4nr | head -n 100 > /root/lot2_top100.tsv

# Ã‰chantillon 5%
python3 - <<'PY'
import random, io
random.seed(42)
lines = [l for l in io.open("/root/lot2_top100.tsv","r",encoding="utf-8",errors="ignore") if l.strip()]
sample = random.sample(lines, min(5, len(lines)))
io.open("/root/lot2_sample5.tsv","w",encoding="utf-8").writelines(sample)
PY